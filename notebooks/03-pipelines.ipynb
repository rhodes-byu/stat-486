{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df531691",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/rhodes-byu/stat-486/blob/main/notebooks/03-pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><p><b></b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c986d0",
   "metadata": {},
   "source": [
    "# scikit-learn Pipelines (Quick Tour)\n",
    "\n",
    "A **pipeline** chains together **preprocessing steps** (transformers) and a final **estimator** (model).\n",
    "\n",
    "Why use pipelines?\n",
    "- **Prevents data leakage**: preprocessing is fit only on training folds during CV.\n",
    "- **Reproducible & concise**: one object handles `fit`, `predict`, and `score` end-to-end.\n",
    "- **Works seamlessly with CV / GridSearchCV**.\n",
    "\n",
    "In scikit-learn, a pipeline looks like:\n",
    "\n",
    "- Step 1..k: **Transformers** that implement `fit` and `transform` (e.g., imputation, scaling, one-hot encoding)\n",
    "- Final step: an **Estimator** that implements `fit` (and typically `predict`, `score`)\n",
    "\n",
    "We'll walk through a few working examples using **KNN** and **Linear Regression**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4782a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Transformers\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Models\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Datasets / synthetic data\n",
    "from sklearn.datasets import load_diabetes, make_regression\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db57394",
   "metadata": {},
   "source": [
    "## Example 1 — StandardScaler → KNN Regression\n",
    "\n",
    "KNN is distance-based, so **feature scaling matters a lot**.\n",
    "We'll use a synthetic regression dataset and compare:\n",
    "- KNN without scaling\n",
    "- KNN with scaling (in a pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba0d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(\n",
    "    n_samples=1200, n_features=20, n_informative=10, noise=25.0, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd230bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=10)\n",
    "\n",
    "pipe_knn_scaled = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"knn\", KNeighborsRegressor(n_neighbors=10)),\n",
    "])\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "pipe_knn_scaled.fit(X_train, y_train)\n",
    "\n",
    "pred = knn.predict(X_test)\n",
    "pred_scaled = pipe_knn_scaled.predict(X_test)\n",
    "\n",
    "print(\"KNN (no scaling)  R^2:\", r2_score(y_test, pred))\n",
    "print(\"KNN (with scaling) R^2:\", r2_score(y_test, pred_scaled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1962d8c",
   "metadata": {},
   "source": [
    "### Cross-validation (why pipelines help)\n",
    "\n",
    "When you cross-validate, you want each fold to learn preprocessing **only from its training split**.\n",
    "Pipelines make that automatic and safe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa3d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(pipe_knn_scaled, X, y, cv=5, scoring=\"r2\")\n",
    "print(\"CV R^2 scores:\", np.round(scores, 3))\n",
    "print(\"Mean CV R^2:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f056fdc",
   "metadata": {},
   "source": [
    "## Example 2 — Imputation → Scaling → Linear Regression\n",
    "\n",
    "We'll use the **diabetes** regression dataset (built into scikit-learn) and *artificially introduce missing values*.\n",
    "Then we build a pipeline that:\n",
    "1. imputes missing values (mean)\n",
    "2. scales features\n",
    "3. fits linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04533d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = load_diabetes()\n",
    "X = diabetes.data.copy()\n",
    "y = diabetes.target.copy()\n",
    "\n",
    "# Introduce ~7% missing values at random\n",
    "missing_mask = np.random.rand(*X.shape) < 0.07\n",
    "X[missing_mask] = np.nan\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "pipe_linreg = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"linreg\", LinearRegression()),\n",
    "])\n",
    "\n",
    "pipe_linreg.fit(X_train, y_train)\n",
    "pred = pipe_linreg.predict(X_test)\n",
    "\n",
    "print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, pred)))\n",
    "print(\"Test R^2:\", r2_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe091590",
   "metadata": {},
   "source": [
    "## Example 3 — Mixed data with ColumnTransformer\n",
    "\n",
    "Real datasets often mix **numeric** and **categorical** columns. `ColumnTransformer` lets you apply\n",
    "different preprocessing pipelines to different column subsets, then combine the results.\n",
    "\n",
    "We'll create a small synthetic dataset with:\n",
    "- numeric columns (with missing values)\n",
    "- categorical columns (with missing values)\n",
    "\n",
    "Then we fit a pipeline ending in **Linear Regression**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e50e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"age\": rng.normal(40, 12, size=n),\n",
    "    \"income\": rng.normal(70000, 20000, size=n),\n",
    "    \"city\": rng.choice([\"NYC\", \"Boston\", \"Chicago\"], size=n, p=[0.5, 0.25, 0.25]),\n",
    "    \"market_segment\": rng.choice([\"A\", \"B\", \"C\"], size=n),\n",
    "})\n",
    "\n",
    "# Target depends on both numeric + categorical effects\n",
    "city_effect = df[\"city\"].map({\"NYC\": 5000, \"Boston\": -2000, \"Chicago\": 1000}).to_numpy()\n",
    "market_segment_effect = df[\"market_segment\"].map({\"A\": 3000, \"B\": 0, \"C\": -1500}).to_numpy()\n",
    "\n",
    "y = (\n",
    "    0.6 * df[\"income\"].to_numpy()\n",
    "    - 200 * df[\"age\"].to_numpy()\n",
    "    + city_effect\n",
    "    + market_segment_effect\n",
    "    + rng.normal(0, 4000, size=n)  # noise\n",
    ")\n",
    "\n",
    "# Introduce missingness\n",
    "for col in [\"age\", \"income\"]:\n",
    "    df.loc[rng.random(n) < 0.05, col] = np.nan\n",
    "for col in [\"city\", \"market_segment\"]:\n",
    "    df.loc[rng.random(n) < 0.05, col] = None\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.25, random_state=42)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da22c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"age\", \"income\"]\n",
    "categorical_features = [\"city\", \"market_segment\"]\n",
    "\n",
    "numeric_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipe, numeric_features),\n",
    "        (\"cat\", categorical_pipe, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_mixed = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"linreg\", LinearRegression()),\n",
    "])\n",
    "\n",
    "pipe_mixed.fit(X_train, y_train)\n",
    "pred = pipe_mixed.predict(X_test)\n",
    "\n",
    "print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, pred)))\n",
    "print(\"Test R^2:\", r2_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e510d",
   "metadata": {},
   "source": [
    "## Example 4 — Modular Design: Swap Models Easily\n",
    "\n",
    "A big win of pipelines: you can **define preprocessing once** and reuse it with different models.\n",
    "This makes model comparison clean and prevents code duplication.\n",
    "\n",
    "We'll compare three models on the same preprocessed data:\n",
    "- Linear Regression\n",
    "- KNN Regression\n",
    "- Ridge Regression (regularized linear model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2e3f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Define preprocessing once (reuse the 'preprocess' from above)\n",
    "# Now create different models with the SAME preprocessing\n",
    "\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"KNN (k=15)\": KNeighborsRegressor(n_neighbors=15),\n",
    "    \"Ridge (alpha=1)\": Ridge(alpha=1),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Create pipeline with same preprocessing, different final estimator\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", model),\n",
    "    ])\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    pred = pipe.predict(X_test)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "    r2 = r2_score(y_test, pred)\n",
    "    \n",
    "    results[model_name] = {\"RMSE\": rmse, \"R^2\": r2}\n",
    "    print(f\"{model_name:20s} | RMSE: {rmse:8.2f} | R^2: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a4ac4a",
   "metadata": {},
   "source": [
    "### Key Benefit: Clean Model Comparison\n",
    "\n",
    "Notice how we:\n",
    "1. **Defined `preprocess` once** (the `ColumnTransformer` from Example 3)\n",
    "2. **Looped through models** and built identical pipelines with just the final estimator swapped\n",
    "3. **No code duplication** for preprocessing logic\n",
    "\n",
    "This pattern scales well when comparing many models or doing hyperparameter tuning with `GridSearchCV`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d8160",
   "metadata": {},
   "source": [
    "## Notes & Common Patterns\n",
    "\n",
    "- Use `Pipeline` for **ordered** steps: `imputer → scaler → model`.\n",
    "- Use `ColumnTransformer` when different columns need different preprocessing.\n",
    "- Hyperparameter tuning works naturally with pipelines (e.g., `GridSearchCV`), using parameter names like:\n",
    "  - `knn__n_neighbors`\n",
    "  - `preprocess__num__imputer__strategy`\n",
    "- Pipelines help ensure **clean evaluation** and simpler production code.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
